{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "### Due: Sat Dec. 8 @ 9pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 4 is composed of two parts.\n",
    "\n",
    "In Part 1, we will see an example of using PCA to plot a high dimensional dataset in 2D. We will then perform K-Means clustering on the dataset and plot using the same PCA representation to see how clustered our data is in the original 64D space.\n",
    "\n",
    "In Part 2 we will generate recommendations on products from a department store based on product descriptions.  \n",
    "We'll first transform the data into topics using Latent Dirichlet Approximation, and then generate recommendations based on this new representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : PCA and K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Use PCA to plot the digits dataset in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The digits dataset is composed of a set of images of handwritten digits 0 to 9.\n",
    "There are 1797 images, each 32x32 pixels.\n",
    "If we flatten out each image we get a dataset of 1797 observations, each with 64 features, each belonging to one of 10 classes.\n",
    "We can't plot them in 64 dimensional space, so we will use PCA to reduce the dimensionality to 2.\n",
    "Hopefully the data will still be clustered by class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "# first we'll load the data\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "X_digits = digits['data']   # grab the digits data (each image a vector, no need to reshape)\n",
    "y_digits = digits['target'] # grab the labels from the digits dataset\n",
    "\n",
    "# show the shape of the dataset (rows,columns)\n",
    "X_digits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using PCA from sklearn.decomposition\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pca object that will result in 2 components being returned\n",
    "#  use random_state=123\n",
    "pca = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a fit_transform on X_digits to get our new X_2d\n",
    "# show the shape of the new X_2d: should be (1797,2)\n",
    "X_2d = ____\n",
    "X_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have the labels, we can plot class separately\n",
    "# for each label (0 to 9), create a scatter plot on the same figure\n",
    "#   use s=80 (size) and alpha=0.8 (too make markers transparent)\n",
    "# we should see that 0 and 1 are far apart, as well as 3 and 4\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for i in range(10):\n",
    "    X_subset = ____\n",
    "    plt.scatter(____)\n",
    "_ = plt.legend(['digit ' + str(x) for x in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How clustered are our classes? Can k-Means find clusters in the 2D PCA transformed data that at all correspond to the plot seen above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using KMeans from sklearn.cluster\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KMeans object which will generate 10 clusters\n",
    "#  use random_state=123\n",
    "km = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use fit_predict() to both fit our k-means model and generate cluster assignments \n",
    "#   on our X_2D dataset with 2 features\n",
    "# show the first 10 cluster assignments\n",
    "# note: cluster assignment values will be from 0 to 9\n",
    "cluster_assignments = ____\n",
    "cluster_assignments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we ignore our y_digit labels.\n",
    "# Create the same plot as we did above, again using the X_2D data,\n",
    "#   but this time one plot per cluster\n",
    "#   useing s=80 (size), alpha=0.8, label='cluster '+str(i)\n",
    "# We're also plotting the cluster centers as X's.\n",
    "# Note that the cluster assignments should look very similar to the class assignments in the plot above, \n",
    "#   meaning that the data is highly clustered even in this 2D space.\n",
    "# Also note that the colors may be different from the plot above, since there is no ordering to the clusters\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for i in range(10):\n",
    "    X_subset = ____\n",
    "    plt.scatter(____)\n",
    "    plt.plot(km.cluster_centers_[i][0],km.cluster_centers_[i][1],marker='x',c='k', ms=20, mew=5, label=None);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : LDA and Recommendation Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform product descriptions into topics and print sample terms from topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create a recommendation engine for products from a department store.  \n",
    "The recommendations will be based on the similarity of product descriptions.  \n",
    "We'll query a product and get back a list of products that are similar.  \n",
    "Instead of using the descriptions directly, we will first do some topic modeling using LDA to transform the descriptions into a topic space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "# import libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "# load product information\n",
    "df_products = pd.read_csv('../data/jcpenney-products_subset.zip',index_col=0)\n",
    "\n",
    "# get product descriptions as our dataset\n",
    "X_products = df_products.description\n",
    "\n",
    "# show the number of rows in the dataset\n",
    "len(X_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a TfidfVectorizer with\n",
    "#   min_df=5, stop_words='english'\n",
    "tfidf = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform tfidf on X, creating the transformed dataset X_tfidf\n",
    "# show the shape of the new dataset\n",
    "X_tfidf = ____\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature names from cv using get_feature_names()\n",
    "# print the first 10 and last 10 features\n",
    "feature_names = ____\n",
    "print(feature_names[:10])\n",
    "print(feature_names[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a LatentDirichletAllocation model with\n",
    "#  n_components=20, n_jobs=-1, random_state=123\n",
    "lda = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform lda on X_tfidf, creating a newly transformed dataset X_lda\n",
    "# show the shape of the new dataset\n",
    "# NOTE: this step may take a minute or more, but no more than 5\n",
    "X_lda = ____\n",
    "X_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "# a utility function to print out the terms that are highly likely for each topic\n",
    "# taken from https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use print_top_words to print the top 10 words for each topic\n",
    "print_top_words(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate recommendations using topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use cosine_similarity to generate our recommendations\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cosine_similarity to generate our similarity scores on our X_lda data\n",
    "# show the shape of the similarities matrix\n",
    "# NOTE: we only need to pass X_lda in once,\n",
    "#  the function will calculate pairwise similarity for all elements in that matrix\n",
    "similarities = ____\n",
    "similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "# we'll get the recommendations for products similar to the one at this index\n",
    "query_idx = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the product name (name_title) for the item at query_idx\n",
    "# NOTE: you need to get this from df_products, perhaps using iloc\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the similarity scores for our query_idx\n",
    "# NOTE: these will be from the query_idx row of similarities\n",
    "query_scores = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to get the indices of our products, sorted by the query_scores\n",
    "# we can do this using np.argsort() which instead of returning a sorted array,\n",
    "#   returns the sorted indices of the array, sorted in ascending value\n",
    "#   eg: x = [0.5,0.7,0.2] => np.argsort(x) = [2,0,1]\n",
    "# show the last 10 values from best_sorted_asc\n",
    "best_sorted_asc = ____\n",
    "best_sorted_asc[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since our best similarity scores are our highest scores, we need to reverse the order\n",
    "# for an np.array() tmp we do this by tmp[::-1]\n",
    "#  eg: x = [1,2,3] => x[::-1] = [3,2,1]\n",
    "# show the first 10 scores of best_sorted_desc (it should be the reverse of the above)\n",
    "best_sorted_desc = ____\n",
    "best_sorted_desc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the the product name for the top 10 recommendations\n",
    "# to do this, for each index in the first 10 elements of best_sorted_desc,\n",
    "#   print the associated df_products.name_title\n",
    "# NOTE: some of these should make sense, some will not\n",
    "#   need to do a little more processing before we deploy...\n",
    "____:\n",
    "    print(____)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coms007",
   "language": "python",
   "name": "coms007"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
