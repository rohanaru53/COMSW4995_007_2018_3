{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "### Due: Tues Nov. 20 @ 9pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will be performing model evaluation, model selection and feature selection in both a regression and classification setting.\n",
    "\n",
    "The data we will be looking at are a subset of home sales data from King County, Washington, as we might see on a real-estate website.\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Follow the comments below and fill in the blanks (____) to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try to build a model to predict adjusted sales price from a set of building features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from file\n",
    "# DO NOT CHANGE THIS (needed for grading)\n",
    "infile_name = '../data/house_sales_subset_normed.csv'\n",
    "df = pd.read_csv(infile_name)\n",
    "\n",
    "# Use a subset of the columns as features\n",
    "X = df[['SqFtTotLiving_norm','SqFtLot_norm','Bathrooms','Bedrooms','TrafficNoise']]\n",
    "\n",
    "# Extract the target, adjusted sale price, in values of $100,000\n",
    "# Note: the '_r' here is denote the different targets for regression and classification\n",
    "y_r = df.AdjSalePrice / 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a held-aside set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into 80% train and 20% test using train_test_split and random_state=42\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train a dummy model on the training set using DummyRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "dummy_r = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print RMSE training set error of the dummy model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "dummy_r_training_rsme = ____\n",
    "print('dummy RMSE: {:.3f}'.format(dummy_r_training_rsme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the R2 training set score of the dummy model\n",
    "# hint: can use models 'score' function\n",
    "dummy_r_training_r2 = ____\n",
    "print('dummy R2: {:.3f}'.format(dummy_r_training_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the mean 5-fold cross valication R2 score of the dummy model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "dummy_r_cv = ____\n",
    "print('dummy mean cv R2: {:.3f}'.format(____))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure performance of Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train a LinearRegression model on the training set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE training set error of the linear model\n",
    "# There should be an improvement over the dummy model\n",
    "lr_rmse = ____\n",
    "print('lr RMSE: {:.3f}'.format(lr_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the R2 training set score of the linear model\n",
    "lr_r2 = ____\n",
    "print('lr R2: {:.4f}'.format(lr_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean 5-fold Cross Validation R2 score of the linear model on the training set using cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = ____\n",
    "print('lr mean cv R2: {:.4f}'.format(____))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll also train an Elastic Net model using regularization\n",
    "# Perform GridSearch over different proportions of the l1_ratio = [.1,.5,.9,1] using the training set\n",
    "# The only parameter in our search is this l1_ratio\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "params = ____\n",
    "gs = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the best R2 score found using grid search and the best parameter setting found\n",
    "print('gs best R2 score : {:.4f}'.format(____))\n",
    "print('gs best params: {}'.format(____))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the best parameter setting found via cross validation in the step before\n",
    "#   calculate and print the mean 5-fold cv R2 score on the training set\n",
    "en = ____\n",
    "scores = ____\n",
    "print('en mean cv R2  : {:.4f}'.format(____))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the ElasticNet model on the full training set and get predictions on the full training set\n",
    "y_hat = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions (x-axis) vs residuals (y-axis) using plt.scatter() with alpha=0.2\n",
    "# Set axis names appropriately ('y_hat' and 'residuals')\n",
    "# recall: residual = y_hat - y\n",
    "residuals = ____\n",
    "_ = ____\n",
    "_ = ____\n",
    "_ = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate trained models on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our trained models, calculate RMSE on the test set\n",
    "print('dummy_r test RMSE  : {:.3f}'.format(____))\n",
    "print('lr test RMSE       : {:.3f}'.format(____))\n",
    "print('en test RMSE       : {:.3f}'.format(____))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the ElasticNet model we trained before, what features have a non-zero coefficient?\n",
    "print('kept columns: {}'.format(____))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, select the top 3 most informative features from the trained model \n",
    "#   using SelectKBest and the f_regression metric\n",
    "# First, instantiate and fit SelectKbest on the training set\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "skb = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the selected features using skb.get_support() and the column names from X_train_r\n",
    "# In this case, they should match the features kept by the ElasticNet model\n",
    "kept_columns = ____\n",
    "print('kept columns: {}'.format(kept_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try to build a model to predict low vs. high adjusted sales price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create classification target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we create a binary target by thresholding at the median of our AdjSalePrice in $100,000\n",
    "y_c = (df.AdjSalePrice > df.AdjSalePrice.median()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the proportion of 'high' labels in our dataset\n",
    "print('proportion of high to low: {:.3f}'.format(____))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a held-aside set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into 80% train and 20% test using train_test_split with random_state=42\n",
    "# Use our new y_c target and the same X we used for regression\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a dummy classification model on the training set\n",
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_c = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training set Accuracy of the dummy classifier\n",
    "# This should be close to the original proportion of low to high\n",
    "dummy_c_acc = ____\n",
    "print('dummy accuracy: {:.3f}'.format(dummy_c_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get P(y=1|x) for the test set using the dummy model (we'll use this later)\n",
    "# Note: we only want P(y=1|x) even though predict_proba returns two columns\n",
    "pypos_dummy = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure performance of a Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train a logistic regression model using default hyperparameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logr = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the training set accuracy of our logistic regression model?\n",
    "trainset_acc = ____\n",
    "print('logr training set accuracy: {:.3f}'.format(trainset_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the 5 fold cross-validation accuracy of the logistic regression model on the training set?\n",
    "scores = ____\n",
    "print('logr mean cv accuracy: {:.3f}'.format(____))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get P(y=1|x) for the test set using the logistic regression model (we'll use this later)\n",
    "pypos_logr = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection using a Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 3-fold cross validated grid search over the number of trees\n",
    "# The parameter settings to try are n_estimators = [5,50,100] \n",
    "# Perform the search using the training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "params = ____\n",
    "gs = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the best score found and the best parameter setting found\n",
    "print('gs best accuracy: {:.3f}'.format(____))\n",
    "print('gs best params  : {}'.format(____))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain on the entire training set using the best number of trees found\n",
    "rf = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get p(y=1|x) for the test set using the trained rf model\n",
    "pypos_rf = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Precision-Recall curve for the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision (y-axis) vs. Recall (x-axis) curve for the Random Forest model\n",
    "# First calculate precision and recall using the y_test_c and pypos_rf \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, _ = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, plot the curve using plt.step()\n",
    "# Recall should be on the x-axis\n",
    "# Label the x and y axes appropriately\n",
    "_ = ____\n",
    "_ = ____\n",
    "_ = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting ROC curves for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curves of our 3 trained models (dummy, logr and rf) \n",
    "# First calculate fpr and tpr for each model using the using y_test_c and each set of pypos values\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr_dummy,tpr_dummy,_ = ____\n",
    "fpr_logr,tpr_logr,_ = ____\n",
    "fpr_rf,tpr_rf,_ = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, plot each of the 3 curves using plt.step()\n",
    "# Each curve should be a different color (dummy:blue, logr:red, rf:green)\n",
    "# Include a legend by adding label='model_name' to each plt.step call and calling plt.legend()\n",
    "# Label the axis as 'fpr' and 'tpr' appropriately\n",
    "_ = ____ # curve for dummy\n",
    "_ = ____ # curve for logr\n",
    "_ = ____ # curve rf\n",
    "_ = ____ # add a legend\n",
    "_ = ____ # set x-axis label\n",
    "_ = ____ # set y-axis label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the ROC AUC values on the test set for each model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "dummy_auc = ____\n",
    "logr_auc = ____\n",
    "rf_auc = ____\n",
    "print('dummy auc: {:.3f}'.format(dummy_auc))\n",
    "print('logr auc : {:.3f}'.format(logr_auc))\n",
    "print('rf auc   : {:.3f}'.format(rf_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the feature importances from the trained Random Forest model, \n",
    "#  print the feature name and feature importances for each feature in X\n",
    "# Each row should look like this, for example: SqFtLot_norm : 0.025\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the most informative features using SelectFromModel using 'mean' as threshold\n",
    "# Use prefit=True since the model is already trained to save needing to retrain\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "sfm = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the selected features using X.columns and sfm.get_support()\n",
    "kept_columns = ____\n",
    "print('kept columns: {}'.format(kept_columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coms007",
   "language": "python",
   "name": "coms007"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
